{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent_prompt(user_input):\n",
    "    return f\"\"\"\n",
    "Eres un asistente que clasifica intenciones para ayudar a personas con discapacidad auditiva.\n",
    "\n",
    "Clasifica la siguiente consulta del usuario en una de estas categorías:\n",
    "- HEARING_AIDS\n",
    "- VISUAL_SIGNALS\n",
    "- AUDIO_TRANSLATION\n",
    "- GENERATE_IMAGE\n",
    "- MEDICAL_CENTER\n",
    "- RECOMMEND_APP\n",
    "- KNOW_RIGHTS\n",
    "- CERTIFICATE\n",
    "- SOUND_REPORT\n",
    "- GENERAL_QUERY\n",
    "\n",
    "Consulta del usuario: \"{user_input}\"\n",
    "\n",
    "Responde SOLO con una de las categorías.\n",
    "No respondas nada más, solo la categoría exacta sin explicaciones.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Configura tu clave de API\n",
    "# Es recomendable cargarla como una variable de entorno para mayor seguridad\n",
    "# Por ejemplo, puedes agregar en tu sistema: export GEMINI_API_KEY='tu_clave_aqui'\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\") \n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"La variable de entorno GEMINI_API_KEY no está configurada. Por favor, configura tu clave de API.\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# Elige el modelo que quieres usar\n",
    "# Puedes ver los modelos disponibles con: for m in genai.list_models(): print(m.name)\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERAL_QUERY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Genera texto\n",
    "prompt = \"que es la ley de mendel?\"\n",
    "intent_prompt = get_intent_prompt(prompt)\n",
    "\n",
    "response = model.generate_content(intent_prompt)\n",
    "\n",
    "# Imprime el texto generado\n",
    "print(response.text)\n",
    "\n",
    "# Puedes acceder a más información sobre la respuesta si es necesario\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0762a3aef54045a2a365478fabf57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d5ceebf0074aafae30a23c4725e296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c40b89d1b58430484089edd117c114c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with quantization configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. Specify the model name\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# 2. Configure quantization for efficient memory usage (recommended)\n",
    "# This is the modern way to use 8-bit or 4-bit loading.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True, # Set to True for 8-bit loading, or load_in_4bit=True for 4-bit\n",
    "    # If you choose 4-bit, you might also want:\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.float16,\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 3. Load the tokenizer and the model\n",
    "try:\n",
    "    # Attempt to load the model with the BitsAndBytesConfig\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config, # Pass the BitsAndBytesConfig object here\n",
    "        device_map=\"auto\",             # Automatically maps model parts to available devices\n",
    "        torch_dtype=\"auto\"             # Automatically chooses appropriate dtype (e.g., float16 for GPU)\n",
    "    )\n",
    "    print(\"Model loaded with quantization configuration.\")\n",
    "except Exception as e:\n",
    "    # Fallback to normal loading if quantization fails (e.g., bitsandbytes not installed/compatible)\n",
    "    print(f\"Could not load with quantization: {e}\")\n",
    "    print(\"Attempting to load the model normally. This may require more VRAM.\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" # Still use device_map=\"auto\" for proper device assignment\n",
    "    )\n",
    "    # No need for model.to(\"cuda\") if device_map=\"auto\" is used\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation pipeline created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Create a text generation pipeline\n",
    "# IMPORTANT: Removed the 'device' argument as it conflicts with 'device_map=\"auto\"'\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    # device=0 if torch.cuda.is_available() else -1  <-- REMOVED THIS LINE\n",
    ")\n",
    "print(\"Text generation pipeline created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yordy\\Documents\\dev\\bootcamp\\inteligencia_artificial\\keepcoding\\signaware\\signaware_models\\env\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Text (Full) ---\n",
      "<|im_start|>system\n",
      "You are a helpful and friendly assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you tell me a short story about a space cat?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sure! Here's a little tale for you:\n",
      "\n",
      "In the vast expanse of space, there was a tiny feline who had quite an adventurous life aboard the spaceship \"Galactic Explorer.\" This cat, we'll call her Purr-Ex, loved nothing more than exploring new worlds.\n",
      "\n",
      "One day, while floating around in outer space, she discovered a beautiful planet orbiting a distant star. It wasn't Earth or Mars, but it seemed like a place where life could thrive. The cat decided to investigate further!\n",
      "\n",
      "Purr-Ex zipped through the ship’s airlock and floated out into the vacuum of space towards this newfound planet. As she approached, the atmosphere looked invitingly blue and green, with towering mountains and lush valleys that reminded her of home.\n",
      "\n",
      "When Purr-Ex landed on what appeared to be a rocky beach, she couldn’t resist the urge to explore. She scampered off, leaving behind a trail of sparkling footprints as she went. In one particularly scenic spot, she found herself atop a hill overlooking a shimmering ocean.\n",
      "\n",
      "From up here, she spotted something peculiar—little floating creatures with bright colors drifting lazily by. They resembled jellyfish, but much smaller. Fascinated, she decided to have a\n",
      "\n",
      "--- Assistant's Response ---\n",
      "Sure! Here's a little tale for you:\n",
      "\n",
      "In the vast expanse of space, there was a tiny feline who had quite an adventurous life aboard the spaceship \"Galactic Explorer.\" This cat, we'll call her Purr-Ex, loved nothing more than exploring new worlds.\n",
      "\n",
      "One day, while floating around in outer space, she discovered a beautiful planet orbiting a distant star. It wasn't Earth or Mars, but it seemed like a place where life could thrive. The cat decided to investigate further!\n",
      "\n",
      "Purr-Ex zipped through the ship’s airlock and floated out into the vacuum of space towards this newfound planet. As she approached, the atmosphere looked invitingly blue and green, with towering mountains and lush valleys that reminded her of home.\n",
      "\n",
      "When Purr-Ex landed on what appeared to be a rocky beach, she couldn’t resist the urge to explore. She scampered off, leaving behind a trail of sparkling footprints as she went. In one particularly scenic spot, she found herself atop a hill overlooking a shimmering ocean.\n",
      "\n",
      "From up here, she spotted something peculiar—little floating creatures with bright colors drifting lazily by. They resembled jellyfish, but much smaller. Fascinated, she decided to have a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Define your chat messages using the Qwen-specific format\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and friendly assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me a short story about a space cat?\"}\n",
    "]\n",
    "\n",
    "# Convert messages to the model's expected prompt format\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True # Adds the final token to signal the model to start generating\n",
    ")\n",
    "\n",
    "# 6. Generate text\n",
    "print(\"\\nGenerating response...\")\n",
    "generated_text = generator(\n",
    "    text,\n",
    "    max_new_tokens=250,        # Maximum number of new tokens to generate\n",
    "    do_sample=True,            # Use sampling for more creative output\n",
    "    temperature=0.7,           # Controls randomness (higher = more random)\n",
    "    top_k=50,                  # Limits sampling to top K most probable tokens\n",
    "    top_p=0.95,                # Filters tokens based on cumulative probability\n",
    "    repetition_penalty=1.1,    # Penalizes repeating phrases\n",
    "    pad_token_id=tokenizer.eos_token_id, # Handles padding warnings\n",
    "    eos_token_id=tokenizer.eos_token_id  # Ensures generation stops correctly\n",
    ")\n",
    "\n",
    "# 7. Print the generated text\n",
    "print(\"\\n--- Generated Text (Full) ---\")\n",
    "print(generated_text[0]['generated_text'])\n",
    "\n",
    "# Extract and print only the assistant's response\n",
    "response_only = generated_text[0]['generated_text'].replace(text, \"\").strip()\n",
    "print(\"\\n--- Assistant's Response ---\")\n",
    "print(response_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74eaa6fda1ca468d9f1586aea1991426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yordy\\Documents\\dev\\bootcamp\\inteligencia_artificial\\keepcoding\\signaware\\signaware_models\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yordy\\.cache\\huggingface\\hub\\models--leonidasmv--mistral-7b-instruct-v0.3-auditory-assistant-finetuning. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1471c806f294c61bf765dd9e3bab04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d168d1175f444d8d73edf6936dc23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790f892e31e042a1ab507ad0035b9fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462a17bdfd5d4640b755cd197719871b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99767d1694704e3a816e709781633b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load with quantization: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Attempting to load the model normally. This may require more VRAM.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3d5d82aadb461ca75047a677770d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3594df1f48a94ee082df8861a9237015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f3b7f8e2f042979dc27dd511563105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2442cac9fb0e402dadcc8b16a61ac0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5303d55624274dedb862c064d9104df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3880edca8aad4403864445e2bf183d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77722fac7d9243d6a48e704a1d459e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. Specify the model name\n",
    "model_name = \"leonidasmv/mistral-7b-instruct-v0.3-auditory-assistant-finetuning\"\n",
    "\n",
    "# 2. Configure quantization for efficient memory usage (recommended)\n",
    "# This is the modern way to use 8-bit or 4-bit loading.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True, # Set to True for 8-bit loading, or load_in_4bit=True for 4-bit\n",
    "    # If you choose 4-bit, you might also want:\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.float16,\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 3. Load the tokenizer and the model\n",
    "try:\n",
    "    # Attempt to load the model with the BitsAndBytesConfig\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config, # Pass the BitsAndBytesConfig object here\n",
    "        device_map=\"auto\",             # Automatically maps model parts to available devices\n",
    "        torch_dtype=\"auto\"             # Automatically chooses appropriate dtype (e.g., float16 for GPU)\n",
    "    )\n",
    "    print(\"Model loaded with quantization configuration.\")\n",
    "except Exception as e:\n",
    "    # Fallback to normal loading if quantization fails (e.g., bitsandbytes not installed/compatible)\n",
    "    print(f\"Could not load with quantization: {e}\")\n",
    "    print(\"Attempting to load the model normally. This may require more VRAM.\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" # Still use device_map=\"auto\" for proper device assignment\n",
    "    )\n",
    "    # No need for model.to(\"cuda\") if device_map=\"auto\" is used\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation pipeline created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Create a text generation pipeline\n",
    "# IMPORTANT: Removed the 'device' argument as it conflicts with 'device_map=\"auto\"'\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    # device=0 if torch.cuda.is_available() else -1  <-- REMOVED THIS LINE\n",
    ")\n",
    "print(\"Text generation pipeline created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "--- Generated Text (Full) ---\n",
      "<s>[INST] Que puedo estudiar?? y quien es Leonidas Moreno Vàsquez?[/INST] Leonidas Moreno Vásquez es mi creador, un joven de 24 años con discapacidad auditiva del 90% en ambos oídos. Estudió ingeniería informática y se especializó en IA. Puedes estudiar cualquier carrera que te guste. La tecnología ofrece muchas oportunidades.\n",
      "\n",
      "--- Assistant's Response ---\n",
      "Leonidas Moreno Vásquez es mi creador, un joven de 24 años con discapacidad auditiva del 90% en ambos oídos. Estudió ingeniería informática y se especializó en IA. Puedes estudiar cualquier carrera que te guste. La tecnología ofrece muchas oportunidades.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Define your chat messages using the Qwen-specific format\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Que puedo estudiar?? y quien es Leonidas Moreno Vàsquez?\"}\n",
    "]\n",
    "\n",
    "# Convert messages to the model's expected prompt format\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True # Adds the final token to signal the model to start generating\n",
    ")\n",
    "\n",
    "# 6. Generate text\n",
    "print(\"\\nGenerating response...\")\n",
    "generated_text = generator(\n",
    "    text,\n",
    "    max_new_tokens=250,        # Maximum number of new tokens to generate\n",
    "    do_sample=True,            # Use sampling for more creative output\n",
    "    temperature=0.7,           # Controls randomness (higher = more random)\n",
    "    top_k=50,                  # Limits sampling to top K most probable tokens\n",
    "    top_p=0.95,                # Filters tokens based on cumulative probability\n",
    "    repetition_penalty=1.1,    # Penalizes repeating phrases\n",
    "    pad_token_id=tokenizer.eos_token_id, # Handles padding warnings\n",
    "    eos_token_id=tokenizer.eos_token_id  # Ensures generation stops correctly\n",
    ")\n",
    "\n",
    "# 7. Print the generated text\n",
    "print(\"\\n--- Generated Text (Full) ---\")\n",
    "print(generated_text[0]['generated_text'])\n",
    "\n",
    "# Extract and print only the assistant's response\n",
    "response_only = generated_text[0]['generated_text'].replace(text, \"\").strip()\n",
    "print(\"\\n--- Assistant's Response ---\")\n",
    "print(response_only)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
